{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import sqlite3\n",
    "from tqdm import tqdm\n",
    "from collections import defaultdict\n",
    "from contextlib import closing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = '../../data/Datasus'\n",
    "\n",
    "uf_dict = defaultdict(lambda: 'Invalid')\n",
    "\n",
    "uf_dict[11] = 'RO'\t\n",
    "uf_dict[12] = 'AC'\t\n",
    "uf_dict[13] = 'AM'\t\n",
    "uf_dict[14] = 'RR'\t\n",
    "uf_dict[15] = 'PA'\t\n",
    "uf_dict[16] = 'AP'\t\n",
    "uf_dict[17] = 'TO'\t\n",
    "uf_dict[21] = 'MA'\t\n",
    "uf_dict[22] = 'PI'\t\n",
    "uf_dict[23] = 'CE'\t\n",
    "uf_dict[24] = 'RN'\t\n",
    "uf_dict[25] = 'PB'\t\n",
    "uf_dict[26] = 'PE'\t\n",
    "uf_dict[27] = 'AL'\t\n",
    "uf_dict[28] = 'SE'\t\n",
    "uf_dict[29] = 'BA'\t\n",
    "uf_dict[31] = 'MG'\t\n",
    "uf_dict[32] = 'ES'\t\n",
    "uf_dict[33] = 'RJ'\t\n",
    "uf_dict[35] = 'SP'\t\n",
    "uf_dict[41] = 'PR'\t\n",
    "uf_dict[42] = 'SC'\t\n",
    "uf_dict[43] = 'RS'\t\n",
    "uf_dict[50] = 'MS'\t\n",
    "uf_dict[51] = 'MT'\t\n",
    "uf_dict[52] = 'GO'\t\n",
    "uf_dict[53] = 'DF'\n",
    "uf_dict[np.nan] = np.nan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "files_list = list(os.listdir(path))\n",
    "\n",
    "files_dict = defaultdict(list)\n",
    "\n",
    "for file in files_list:\n",
    "    if 'ANIMBR' in file: files_dict['ANIMBR'].append(file)\n",
    "    elif 'CHAGBR' in file: files_dict['CHAGBR'].append(file)\n",
    "    elif 'CHIKBR' in file: files_dict['CHIKBR'].append(file)\n",
    "    elif 'DENGBR' in file: files_dict['DENGBR'].append(file)\n",
    "    elif 'ESQUBR' in file: files_dict['ESQUBR'].append(file)\n",
    "    elif 'HANSBR' in file: files_dict['HANSBR'].append(file)\n",
    "    elif 'LEIVBR' in file: files_dict['LEIVBR'].append(file)\n",
    "    elif 'LTANBR' in file: files_dict['LTANBR'].append(file)\n",
    "    elif 'RAIVBR' in file: files_dict['RAIVBR'].append(file)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Structure Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this step the structure of each csv was analysed, considering that the data pertaining to the same condition is planned to be stacked into a single database table first it was necessary to check if the data between each data source is consistent.\n",
    "\n",
    "Unfortunately, exploration made clear that this consistent is not a guarantee. Therefore, a second step was made in which all possible columns for each condition were extracted in a dictionary and stored as a json to help with the <ins>Insertion</ins> step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_column_integrity(data_list:list, path:str=path, low_memory:bool=False) -> bool:\n",
    "\n",
    "    base_columns = list(pd.read_csv(f'{path}/{data_list[0]}', encoding='ISO-8859-1', low_memory=low_memory, skiprows=lambda x: x not in [0]).columns)\n",
    "\n",
    "    for file in data_list:\n",
    "        column_list = list(pd.read_csv(f'{path}/{file}', encoding='ISO-8859-1', low_memory=low_memory, skiprows=lambda x: x not in [0]).columns)\n",
    "\n",
    "        if not set(base_columns) == set(column_list):\n",
    "            print(f' -- File Base = {data_list[0]}')\n",
    "            print(f' -- File Falha = {file}')\n",
    "            print(f'Diferenças 1: {set(base_columns) - set(column_list)}')\n",
    "            print(f'Diferenças 2: {set(column_list) - set(base_columns)}')\n",
    "\n",
    "            return False\n",
    "        \n",
    "    return True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" for key in files_dict:\n",
    "    #if not key == 'DENGBR':\n",
    "        print(f'{key}')\n",
    "        print(f' -- {check_column_integrity(data_list=files_dict[key])}')\n",
    "        print() \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/25 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 25/25 [01:12<00:00,  2.90s/it]\n",
      "100%|██████████| 17/17 [00:00<00:00, 25.32it/s]\n",
      "100%|██████████| 17/17 [00:10<00:00,  1.58it/s]\n",
      "100%|██████████| 15/15 [00:00<00:00, 71.88it/s]\n",
      "100%|██████████| 24/24 [00:03<00:00,  7.26it/s]\n",
      "100%|██████████| 22/22 [00:00<00:00, 37.36it/s]\n",
      "100%|██████████| 23/23 [00:02<00:00, 10.56it/s]\n",
      "100%|██████████| 23/23 [00:01<00:00, 22.43it/s]\n",
      "100%|██████████| 10/10 [00:08<00:00,  1.23it/s]\n"
     ]
    }
   ],
   "source": [
    "full_columns = {}\n",
    "for key in files_dict:\n",
    "    all_columns = set()\n",
    "\n",
    "    for file in tqdm(files_dict[key], total=len(files_dict[key])):\n",
    "        all_columns |= set(list(pd.read_csv(f'{path}/{file}', encoding='ISO-8859-1', low_memory=False, skiprows=lambda x: x not in [0]).columns))\n",
    "\n",
    "    full_columns[key] = all_columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for key in full_columns: full_columns[key] = list(full_columns[key])\n",
    "with open(\"../../data/Datasus/0_full_columns.json\", 'w') as f: json.dump(full_columns, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Insertion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_instance(file:str, full_columns:set, name:str, con:sqlite3.Connection, aux_dict:dict=uf_dict, path:str=path, low_memory:bool=False) -> None:\n",
    "    chunksize = 10 ** 6\n",
    "\n",
    "    with pd.read_csv(f'{path}/{file}', encoding='ISO-8859-1', low_memory=low_memory, chunksize=chunksize) as reader:\n",
    "\n",
    "            for chunk in reader:\n",
    "                columns = set(list(chunk.columns))\n",
    "                \n",
    "                \n",
    "                to_update = [column for column in columns if 'UF' in column and pd.api.types.is_any_real_numeric_dtype(chunk[column])]\n",
    "                for column in to_update:\n",
    "                    chunk[column] = chunk[column].map(aux_dict)\n",
    "\n",
    "                \n",
    "                columns_to_create = full_columns - columns\n",
    "\n",
    "                for column in columns_to_create: \n",
    "                    chunk[column] = np.nan\n",
    "\n",
    "\n",
    "                chunk.to_sql(name=name, con=con, if_exists='append')\n",
    "\n",
    "\n",
    "def process_data(data_list:list, full_columns:set, name:str, con:sqlite3.Connection, aux_dict:dict=uf_dict, path:str=path, low_memory:bool=False) -> None:\n",
    "    \n",
    "    for file in tqdm(data_list, total=len(data_list)):\n",
    "        process_instance(file=file, full_columns=full_columns, name=name, con=con)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Insertions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I made the choice to insert the data of each table individually considering the possibility of a long execution time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"../../data/Datasus/0_full_columns.json\", 'r') as f:\n",
    "        full_columns = json.load(f)\n",
    "\n",
    "con = sqlite3.connect(\"../../data/base.db\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# process_data(data_list=files_dict['ANIMBR'], full_columns=set(full_columns['ANIMBR']), name='peconhento_datasus', con=con)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# process_data(data_list=files_dict['CHIKBR'], full_columns=set(full_columns['CHIKBR']), name='chikungunya_datasus', con=con)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# process_data(data_list=files_dict['RAIVBR'], full_columns=set(full_columns['RAIVBR']), name='raiva_datasus', con=con)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# process_data(data_list=files_dict['HANSBR'], full_columns=set(full_columns['HANSBR']), name='hanseniase_datasus', con=con)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# process_data(data_list=files_dict['CHAGBR'], full_columns=set(full_columns['CHAGBR']), name='chagas_datasus', con=con)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# process_data(data_list=files_dict['ESQUBR'], full_columns=set(full_columns['ESQUBR']), name='esquistossomose_datasus', con=con)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# process_data(data_list=files_dict['DENGBR'], full_columns=set(full_columns['DENGBR']), name='dengue_datasus', con=con)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# process_data(data_list=files_dict['LEIVBR'], full_columns=set(full_columns['LEIVBR']), name='leish_viceral_datasus', con=con)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# process_data(data_list=files_dict['LTANBR'], full_columns=set(full_columns['LTANBR']), name='leish_tegumentar_datasus', con=con)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Close the connection\n",
    "con.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
